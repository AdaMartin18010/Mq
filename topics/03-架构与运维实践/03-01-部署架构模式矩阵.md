# 3.1 部署架构模式矩阵

## 目录

- [3.1 部署架构模式矩阵](#31-部署架构模式矩阵)
  - [目录](#目录)
  - [3.1.1 部署架构模式对比](#311-部署架构模式对比)
  - [3.1.2 高可用架构设计模式](#312-高可用架构设计模式)
    - [Kafka高可用模式](#kafka高可用模式)
    - [MQTT高可用模式](#mqtt高可用模式)
    - [NATS高可用模式](#nats高可用模式)
  - [3.1.3 架构权衡证明](#313-架构权衡证明)

---

## 3.1.1 部署架构模式对比

| 维度 | Kafka | MQTT | NATS Core | NATS JetStream | 架构决策依据 |
|------|-------|------|-----------|----------------|--------------|
| **部署形态** | 有状态集群 | 有状态集群/单机 | 无状态集群 | 有状态集群 | **关键差异**：Kafka和JetStream必须管理磁盘状态，而NATS Core可无状态部署 |
| **最少生产节点** | 3 Broker+3 ZK | 2 Broker(主从) | 1 Server | 3 Server | **成本起点**：NATS Core可从单节点起步，Kafka最低需要6个进程 |
| **网络拓扑** | 客户端-Broker直连 | 客户端-Broker直连 | 全网状自发现 | 全网状自发现 | **运维复杂度**：NATS的Gossip协议自动发现，无需负载均衡器 |
| **存储依赖** | 本地磁盘+ZooKeeper | 本地磁盘/Memory | 纯内存 | 本地磁盘 | **可靠性**：Kafka依赖ZK元数据，运维需维护两套集群 |
| **云原生适配** | StatefulSet+LocalPV | StatefulSet | DaemonSet/StatefulSet | StatefulSet | **K8s部署**：NATS Core适合DaemonSet每个节点部署，降低网络延迟 |

## 3.1.2 高可用架构设计模式

### Kafka高可用模式

```
┌─────────────────────────────────────────┐
│          客户端层(Producer/Consumer)     │
└──────────────┬──────────────────────────┘
               │
               ↓
┌─────────────────────────────────────────┐
│      VIP/LoadBalancer(HAProxy)          │
│       (故障时IP漂移)                    │
└──────────────┬──────────────────────────┘
               │
               ↓
┌─────────────────────────────────────────┐
│  Broker层(3节点)                        │
│  ┌─────────┐ ┌─────────┐ ┌─────────┐  │
│  │Broker-1 │ │Broker-2 │ │Broker-3 │  │
│  │Controller│ │Follower │ │Follower │  │
│  └────┬────┘ └────┬────┘ └────┬────┘  │
│       │           │           │        │
│       └──────┬────┴────┬──────┘        │
│              ↓         ↓               │
│     ┌─────────────────────────┐        │
│     │  ZooKeeper集群(3节点)    │        │
│     │  (元数据+Controller选举) │        │
│     └─────────────────────────┘        │
└─────────────────────────────────────────┘

架构要点：
1. ZK必须是3/5节点奇数集群，与Broker物理隔离
2. Broker的log.dirs必须挂载在独立磁盘，避免与系统盘IO竞争
3. 每个Topic分区副本必须跨机架部署：replica.fetch.max.bytes需调优
4. 监控重点：UnderReplicatedPartitions、ISR shrink rate
```

### MQTT高可用模式

```
┌─────────────────────────────────────────┐
│         设备层(10万+设备)               │
└──────────────┬──────────────────────────┘
               │
               ↓
┌─────────────────────────────────────────┐
│         MQTT Broker集群(2节点)          │
│  ┌─────────────┐      ┌─────────────┐  │
│  │Master Broker│◄────►│Slave Broker │  │
│  │(Active)     │ 复制 │(Standby)    │  │
│  └──────┬──────┘      └──────┬──────┘  │
│         │                    │         │
│         └──────────┬─────────┘         │
│                    ↓                   │
│         ┌──────────────────┐         │
│         │ 共享存储(NFS)     │         │
│         │ 会话+消息持久化   │         │
│         └──────────────────┘         │
└─────────────────────────────────────────┘

架构要点：
1. 主从模式通过共享存储实现会话状态同步
2. 设备端必须实现断线重连+会话恢复机制
3. 网关心跳检测：keepalive=60s，避免设备假在线
4. 监控重点：ConnectedClients、MessageDropRate
```

### NATS高可用模式

```
┌─────────────────────────────────────────┐
│         客户端层(多语言SDK)              │
│   内置连接池+自动重连+服务发现           │
└──────────────┬──────────────────────────┘
               │
               ↓
┌─────────────────────────────────────────┐
│      NATS集群(3节点，无中心)            │
│      ┌─────────┐ ┌─────────┐ ┌─────────┐ │
│      │Server-1 │ │Server-2 │ │Server-3 │ │
│      │(Router) │ │(Router) │ │(Router) │ │
│      └─────┬───┘ └─────┬───┘ └─────┬───┘ │
│            │           │           │     │
│            └─────┬─────┴─────┬─────┘     │
│                  ↓           ↓           │
│            ┌──────────────────────┐     │
│            │  Gossip协议自动发现    │     │
│            │  无外部依赖            │     │
│            └──────────────────────┘     │
└─────────────────────────────────────────┘

架构要点：
1. 无需负载均衡器，客户端配置多个server地址自动切换
2. 无状态设计，任何节点故障不影响其他节点
3. JetStream模式下需配置raft集群存储组，最少3节点
4. 监控重点：Routes数量、SlowConsumers、JetStream RAFT状态
```

## 3.1.3 架构权衡证明

```
定理：在云原生环境下，NATS运维复杂度显著低于Kafka

证明：
设运维操作集合 O = {部署, 扩缩容, 故障恢复, 升级}

对于Kafka：
- 部署：需先部署ZooKeeper集群，再部署Broker集群，配置server.properties
- 扩缩容：增加Broker后需手动执行分区重分配(reassign)
- 故障恢复：Controller选举需10-30秒，期间部分分区不可用
- 升级：需逐台滚动升级，注意协议版本兼容性

对于NATS Core：
- 部署：单个二进制文件，无配置文件，通过命令行参数启动
- 扩缩容：新节点自动加入集群，客户端自动重连
- 故障恢复：客户端秒级重连到其他节点，无中心节点选举
- 升级：替换二进制文件，重启即可

∴ complexity(NATS) << complexity(Kafka)
```

## 3.1.4 部署架构参考资源

### Kafka部署参考

- **官方部署指南**: [Kafka Deployment](https://kafka.apache.org/documentation/#deployment)
- **KRaft模式**: [KRaft Mode](https://kafka.apache.org/documentation/#kraft)
- **Kubernetes部署**: [Kafka on Kubernetes](https://strimzi.io/)

### MQTT部署参考

- **EMQX部署**: [EMQX Deployment Guide](https://www.emqx.io/docs/en/latest/deploy/)
- **AWS IoT Core**: [AWS IoT Core Deployment](https://docs.aws.amazon.com/iot/latest/developerguide/what-is-aws-iot.html)

### NATS部署参考

- **官方部署指南**: [NATS Deployment](https://docs.nats.io/running-a-nats-service/introduction)
- **Kubernetes部署**: [NATS on Kubernetes](https://docs.nats.io/running-a-nats-service/nats-on-kubernetes)
- **Docker部署**: [NATS Docker](https://hub.docker.com/_/nats)

---

**参考来源**:

- 基于concept03.md内容整理
- Kafka、MQTT、NATS官方部署文档
- Kubernetes和Docker部署最佳实践
- 生产环境部署案例
